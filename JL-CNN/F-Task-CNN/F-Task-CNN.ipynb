{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n",
      "/home/c1501f/anaconda3/envs/tensorflow/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:519: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
      "/home/c1501f/anaconda3/envs/tensorflow/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:520: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
      "/home/c1501f/anaconda3/envs/tensorflow/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:521: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
      "/home/c1501f/anaconda3/envs/tensorflow/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:522: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
      "/home/c1501f/anaconda3/envs/tensorflow/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:523: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
      "/home/c1501f/anaconda3/envs/tensorflow/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:528: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import sys\n",
    "import random\n",
    "import warnings\n",
    "import math\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt \n",
    "\n",
    "from tqdm import tqdm\n",
    "from itertools import chain\n",
    "\n",
    "from keras.utils import to_categorical\n",
    "from keras.callbacks import Callback\n",
    "\n",
    "from keras import layers\n",
    "from keras import optimizers\n",
    "from keras.models import Model, load_model\n",
    "from keras.layers import Input, Activation, Dense, Flatten\n",
    "from keras.layers import Concatenate, multiply\n",
    "from keras.layers import Dropout\n",
    "from keras.layers import BatchNormalization\n",
    "from keras.layers import Conv1D\n",
    "from keras.layers import ZeroPadding1D\n",
    "from keras.layers import GlobalAveragePooling1D\n",
    "from keras.layers.pooling import MaxPooling1D\n",
    "\n",
    "from keras.layers import Deconv2D,LeakyReLU,add,Reshape\n",
    "from keras import losses\n",
    "\n",
    "from keras import regularizers\n",
    "from keras.layers import Conv2D\n",
    "\n",
    "from keras.losses import categorical_crossentropy\n",
    "\n",
    "from keras.layers.core import Lambda\n",
    "\n",
    "from keras.layers.merge import concatenate\n",
    "from keras.callbacks import EarlyStopping, ModelCheckpoint, ReduceLROnPlateau\n",
    "\n",
    "from sklearn.metrics import accuracy_score,f1_score,recall_score,precision_score\n",
    "\n",
    "from keras.models import model_from_json\n",
    "\n",
    "from keras import  backend as K\n",
    "\n",
    "import tensorflow as tf\n",
    "\n",
    "\n",
    "\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0\"\n",
    "\n",
    "\n",
    "import keras.backend.tensorflow_backend as KTF\n",
    "\n",
    "config = tf.ConfigProto()\n",
    "config.gpu_options.per_process_gpu_memory_fraction = 0.1\n",
    "session = tf.Session(config=config)\n",
    "\n",
    "KTF.set_session(session )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_batch = 128\n",
    "test_batch = 128\n",
    "\n",
    "train_num = 40020\n",
    "test_num = 13108\n",
    "\n",
    "cross = \"cross_1\"\n",
    "\n",
    "snr_str = \"-6_db\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_train_data():\n",
    "    \n",
    "    train_data = np.zeros((train_batch, 2048, 1), dtype = np.float32)\n",
    "    d_train_lab = np.zeros((train_batch, 2048, 1), dtype = np.float32)\n",
    "    c_train_lab = np.zeros((train_batch))\n",
    "    \n",
    "    flag = 0\n",
    "    \n",
    "    while True:\n",
    "        \n",
    "        list = random.sample(range(train_num), train_num)\n",
    "        \n",
    "        for id in list:\n",
    "            \n",
    "            num_id = str(id)\n",
    "            \n",
    "            np_train_data = np.load(\"../../../Data/Xi_W_all/denoise_data/\"+cross+\"/\"+snr_str+\"/train_data/\" + num_id + \"_train.npy\")\n",
    "            npd_train_lab = np.load(\"../../../Data/Xi_W_all/denoise_data/\"+cross+\"/no_db/train_data/\" + num_id + \"_train.npy\") \n",
    "            npc_train_lab = np.load(\"../../../Data/Xi_W_all/denoise_data/\"+cross+\"/\"+snr_str+\"/train_lab/\" + num_id + \"_lab.npy\")\n",
    "            \n",
    "            train_data[flag, :, :] = np_train_data\n",
    "            d_train_lab[flag, :, :] = npd_train_lab    \n",
    "            c_train_lab[flag] = npc_train_lab\n",
    "            \n",
    "            flag += 1\n",
    "            \n",
    "            if flag >= train_batch:\n",
    "                \n",
    "                train_hot_lab = to_categorical(c_train_lab, num_classes=10)\n",
    "                \n",
    "                yield [train_data], [train_hot_lab]\n",
    "                \n",
    "                flag = 0\n",
    "                train_data = np.zeros((train_batch, 2048, 1), dtype = np.float32)\n",
    "                d_train_lab = np.zeros((train_batch, 2048, 1), dtype = np.float32)\n",
    "                c_train_lab = np.zeros((train_batch))\n",
    "\n",
    "        \n",
    "def generate_test_data():\n",
    "    \n",
    "    test_data = np.zeros((test_batch, 2048, 1), dtype = np.float32)\n",
    "    d_test_lab = np.zeros((test_batch, 2048, 1), dtype = np.float32)\n",
    "    c_test_lab = np.zeros((test_batch))\n",
    "    \n",
    "    flag = 0\n",
    "    \n",
    "    while True:\n",
    "        \n",
    "        list = random.sample(range(test_num), test_num)\n",
    "        \n",
    "        for id in list:\n",
    "            \n",
    "            num_id = str(id)\n",
    "            \n",
    "            np_test_data = np.load(\"../../../Data/Xi_W_all/denoise_data/\"+cross+\"/\"+snr_str+\"/test_data/\" + num_id + \"_test.npy\")\n",
    "            npd_test_lab = np.load(\"../../../Data/Xi_W_all/denoise_data/\"+cross+\"/no_db/test_data/\" + num_id + \"_test.npy\") \n",
    "            npc_test_lab = np.load(\"../../../Data/Xi_W_all/denoise_data/\"+cross+\"/\"+snr_str+\"/test_lab/\" + num_id + \"_lab.npy\")\n",
    "            \n",
    "            test_data[flag, :, :] = np_test_data\n",
    "            d_test_lab[flag, :, :] = npd_test_lab    \n",
    "            c_test_lab[flag] = npc_test_lab\n",
    "            \n",
    "            flag += 1\n",
    "            \n",
    "            if flag >= test_batch:\n",
    "                \n",
    "                test_hot_lab = to_categorical(c_test_lab, num_classes=10)\n",
    "                \n",
    "                yield [test_data], [test_hot_lab]\n",
    "                \n",
    "                flag = 0\n",
    "                test_data = np.zeros((test_batch, 2048, 1), dtype = np.float32)\n",
    "                d_test_lab = np.zeros((test_batch, 2048, 1), dtype = np.float32)\n",
    "                c_test_lab = np.zeros((test_batch))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_all_test_data():\n",
    "    \n",
    "    test_data = np.zeros((test_num, 2048, 1), dtype = np.float32)\n",
    "    d_test_lab = np.zeros((test_num, 2048, 1), dtype = np.float32)\n",
    "    c_test_lab = np.zeros((test_num))\n",
    "    \n",
    "    flag = 0\n",
    "    list = random.sample(range(test_num), test_num)\n",
    "    \n",
    "    for id in list:\n",
    "        \n",
    "        num_id = str(id)\n",
    "        \n",
    "        np_test_data = np.load(\"../../../Data/Xi_W_all/denoise_data/\"+cross+\"/\"+snr_str+\"/test_data/\" + num_id + \"_test.npy\")\n",
    "        npd_test_lab = np.load(\"../../../Data/Xi_W_all/denoise_data/\"+cross+\"/no_db/test_data/\" + num_id + \"_test.npy\") \n",
    "        npc_test_lab = np.load(\"../../../Data/Xi_W_all/denoise_data/\"+cross+\"/\"+snr_str+\"/test_lab/\" + num_id + \"_lab.npy\")\n",
    "            \n",
    "        test_data[flag, :, :] = np_test_data\n",
    "        d_test_lab[flag, :, :] = npd_test_lab    \n",
    "        c_test_lab[flag] = npc_test_lab\n",
    "        \n",
    "        flag += 1\n",
    "        \n",
    "    test_hot_lab = to_categorical(c_test_lab, num_classes=10)\n",
    "    \n",
    "    return test_data, d_test_lab, test_hot_lab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "slope = 0.3\n",
    "\n",
    "def Denosing_CNN2():\n",
    "    \n",
    "    input = Input(shape=[2048, 1])\n",
    "\n",
    "    D1 = Conv1D(filters=16, kernel_size=12, strides=4, padding=\"same\")(input)\n",
    "    D1 = LeakyReLU(alpha=slope)(D1)\n",
    "        \n",
    "    D2 = Conv1D(filters=32, kernel_size=6, strides=4, padding=\"same\")(D1)\n",
    "    D2 = LeakyReLU(alpha=slope)(D2)    \n",
    "    \n",
    "    D3 = Conv1D(filters=64, kernel_size=3, strides=2, padding=\"same\")(D2)\n",
    "    D3 = LeakyReLU(alpha=slope)(D3)\n",
    "    \n",
    "    D4 = Conv1D(filters=128, kernel_size=3, strides=2, padding=\"same\")(D3)\n",
    "    D4 = LeakyReLU(alpha=slope)(D4)    \n",
    "      \n",
    "\n",
    "    Fx_5 = Conv1D(256, kernel_size = 3, strides=2, padding='same')(D4)\n",
    "    Fx_5 = LeakyReLU(alpha=slope)(Fx_5)   \n",
    "    \n",
    "    Fx_6 = GlobalAveragePooling1D()(Fx_5)\n",
    "    Fx_6 = Dense(10,  activation='softmax', name='out2')(Fx_6)     \n",
    " \n",
    "         \n",
    "    Total_Model = Model(inputs=input, outputs= Fx_6)  \n",
    "    \n",
    "    return Total_Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_1 (InputLayer)         (None, 2048, 1)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_1 (Conv1D)            (None, 512, 16)           208       \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_1 (LeakyReLU)    (None, 512, 16)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_2 (Conv1D)            (None, 128, 32)           3104      \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_2 (LeakyReLU)    (None, 128, 32)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_3 (Conv1D)            (None, 64, 64)            6208      \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_3 (LeakyReLU)    (None, 64, 64)            0         \n",
      "_________________________________________________________________\n",
      "conv1d_4 (Conv1D)            (None, 32, 128)           24704     \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_4 (LeakyReLU)    (None, 32, 128)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_5 (Conv1D)            (None, 16, 256)           98560     \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_5 (LeakyReLU)    (None, 16, 256)           0         \n",
      "_________________________________________________________________\n",
      "global_average_pooling1d_1 ( (None, 256)               0         \n",
      "_________________________________________________________________\n",
      "out2 (Dense)                 (None, 10)                2570      \n",
      "=================================================================\n",
      "Total params: 135,354\n",
      "Trainable params: 135,354\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "adam = optimizers.adam(lr = 0.0001)\n",
    "\n",
    "model = Denosing_CNN2()\n",
    "\n",
    "model.compile(optimizer = 'adam', loss='categorical_crossentropy', metrics = ['accuracy'])\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n"
     ]
    }
   ],
   "source": [
    "class MyCbk(Callback):\n",
    "\n",
    "    def __init__(self, model):\n",
    "        self.model_to_save = model\n",
    "            \n",
    "    def on_epoch_end(self, epoch, logs=None):\n",
    "        \n",
    "        print('save model----model_at_epoch_%d.h5' % epoch)\n",
    "        self.model_to_save.save('model/8/model_%d.h5' % epoch)\n",
    "        \n",
    "cbk = MyCbk(model)\n",
    "\n",
    "reduceLR = ReduceLROnPlateau(monitor='val_loss', factor=0.90, patience=5, verbose=0, mode='auto', cooldown=0, min_lr=0.0000001)\n",
    "\n",
    "results = model.fit_generator(generate_train_data(), steps_per_epoch = train_num/train_batch, epochs = 100, validation_data = generate_test_data(), validation_steps = test_num/test_batch, verbose=1, callbacks=[cbk, reduceLR])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_test_data, all_d_lab, all_hot_lab = get_all_test_data()\n",
    "\n",
    "\n",
    "Accuracy_list = np.zeros((100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for model_id in range(99, 100):\n",
    "    \n",
    "    str_id = str(model_id)\n",
    "    model =  load_model('model/8/model_'+ str_id +'.h5') \n",
    "    \n",
    "    range_num = int(test_num/test_batch)\n",
    "    \n",
    "    input_hot_lab = []\n",
    "    output_hot_lab = []   \n",
    "    \n",
    "    for num_id in range(range_num):\n",
    "        \n",
    "        test_data = all_test_data[num_id*test_batch:test_batch+num_id*test_batch,:,:]\n",
    "        hot_lab = all_hot_lab[num_id*test_batch:test_batch+num_id*test_batch,:]\n",
    "    \n",
    "        pre_hot = model.predict(test_data, batch_size=test_batch, verbose=0)\n",
    "           \n",
    "        input_hot_lab.extend(hot_lab)\n",
    "        output_hot_lab.extend(pre_hot)\n",
    "        \n",
    "    hot_labels = np.argmax(input_hot_lab,axis=1)  \n",
    "    pre_hot_labels = np.argmax(output_hot_lab,axis=1)\n",
    "          \n",
    "    Accuracy = accuracy_score(hot_labels, pre_hot_labels)\n",
    "    Accuracy_list[model_id] = Accuracy\n",
    "    \n",
    "    print(\"第\"+ str(model_id) + \"个模型的结果：\")\n",
    "    print(Accuracy)\n",
    "    \n",
    "    print(\"------------------------------------\")\n",
    "    \n",
    "    K.clear_session()\n",
    "    tf.reset_default_graph()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Accuracy_str = \"[\"\n",
    "\n",
    "for id in range(80, 100):\n",
    "    \n",
    "    Accuracy_num = round(Accuracy_list[id], 5)\n",
    "\n",
    "    Accuracy_str = Accuracy_str + str(Accuracy_num)\n",
    "    \n",
    "    if id == 99:\n",
    "        Accuracy_str = Accuracy_str + \"]\"\n",
    "\n",
    "    else:\n",
    "        Accuracy_str = Accuracy_str + \", \"\n",
    "\n",
    "print(\"Accuracy_score:\")\n",
    "print(Accuracy_str)\n",
    "\n",
    "\n",
    "print('Accuracy: '+str(Accuracy_list[80:99].max()))\n",
    "\n",
    "print(\"**------------**-------------**---------------**\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python tensorflow",
   "language": "python",
   "name": "tensorflow"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
