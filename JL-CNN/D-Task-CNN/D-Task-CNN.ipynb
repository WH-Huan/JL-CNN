{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import random\n",
    "import warnings\n",
    "import math\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt \n",
    "\n",
    "from tqdm import tqdm\n",
    "from itertools import chain\n",
    "\n",
    "from keras.utils import to_categorical\n",
    "from keras.callbacks import Callback\n",
    "\n",
    "from keras import layers\n",
    "from keras import optimizers\n",
    "from keras.models import Model, load_model\n",
    "from keras.layers import Input, Activation, Dense, Flatten\n",
    "from keras.layers import Concatenate, multiply\n",
    "from keras.layers import Dropout\n",
    "from keras.layers import BatchNormalization\n",
    "from keras.layers import Conv1D\n",
    "from keras.layers import ZeroPadding1D\n",
    "from keras.layers import GlobalAveragePooling1D\n",
    "from keras.layers.pooling import MaxPooling1D\n",
    "\n",
    "from keras.layers import Deconv2D,LeakyReLU,add,Reshape\n",
    "from keras import losses\n",
    "\n",
    "from keras import regularizers\n",
    "from keras.layers import Conv2D\n",
    "\n",
    "from keras.losses import categorical_crossentropy\n",
    "\n",
    "from keras.layers.core import Lambda\n",
    "\n",
    "from keras.layers.merge import concatenate\n",
    "from keras.callbacks import EarlyStopping, ModelCheckpoint, ReduceLROnPlateau\n",
    "\n",
    "from sklearn.metrics import accuracy_score,f1_score,recall_score,precision_score\n",
    "\n",
    "from keras.models import model_from_json\n",
    "\n",
    "from keras import  backend as K\n",
    "\n",
    "import tensorflow as tf\n",
    "\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"1\"\n",
    "\n",
    "import keras.backend.tensorflow_backend as KTF\n",
    "\n",
    "config = tf.ConfigProto()\n",
    "config.gpu_options.per_process_gpu_memory_fraction = 0.1\n",
    "session = tf.Session(config=config)\n",
    "\n",
    "KTF.set_session(session )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_batch = 128\n",
    "test_batch = 128\n",
    "\n",
    "train_num = 40020\n",
    "test_num = 13108\n",
    "\n",
    "cross = \"cross_1\"\n",
    "\n",
    "snr_str = \"-6_db\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_train_data():\n",
    "    \n",
    "    train_data = np.zeros((train_batch, 2048, 1), dtype = np.float32)\n",
    "    d_train_lab = np.zeros((train_batch, 2048, 1), dtype = np.float32)\n",
    "    c_train_lab = np.zeros((train_batch))\n",
    "    \n",
    "    flag = 0\n",
    "    \n",
    "    while True:\n",
    "        \n",
    "        list = random.sample(range(train_num), train_num)\n",
    "        \n",
    "        for id in list:\n",
    "            \n",
    "            num_id = str(id)\n",
    "            \n",
    "            np_train_data = np.load(\"../../../Data/Xi_W_all/denoise_data/\"+cross+\"/\"+snr_str+\"/train_data/\" + num_id + \"_train.npy\")\n",
    "            npd_train_lab = np.load(\"../../../Data/Xi_W_all/denoise_data/\"+cross+\"/no_db/train_data/\" + num_id + \"_train.npy\") \n",
    "            npc_train_lab = np.load(\"../../../Data/Xi_W_all/denoise_data/\"+cross+\"/\"+snr_str+\"/train_lab/\" + num_id + \"_lab.npy\")\n",
    "            \n",
    "            train_data[flag, :, :] = np_train_data\n",
    "            d_train_lab[flag, :, :] = npd_train_lab    \n",
    "            c_train_lab[flag] = npc_train_lab\n",
    "            \n",
    "            flag += 1\n",
    "            \n",
    "            if flag >= train_batch:\n",
    "                \n",
    "                train_hot_lab = to_categorical(c_train_lab, num_classes=10)\n",
    "                \n",
    "                yield [train_data], [d_train_lab]\n",
    "                \n",
    "                flag = 0\n",
    "                train_data = np.zeros((train_batch, 2048, 1), dtype = np.float32)\n",
    "                d_train_lab = np.zeros((train_batch, 2048, 1), dtype = np.float32)\n",
    "                c_train_lab = np.zeros((train_batch))\n",
    "\n",
    "        \n",
    "def generate_test_data():\n",
    "    \n",
    "    test_data = np.zeros((test_batch, 2048, 1), dtype = np.float32)\n",
    "    d_test_lab = np.zeros((test_batch, 2048, 1), dtype = np.float32)\n",
    "    c_test_lab = np.zeros((test_batch))\n",
    "    \n",
    "    flag = 0\n",
    "    \n",
    "    while True:\n",
    "        \n",
    "        list = random.sample(range(test_num), test_num)\n",
    "        \n",
    "        for id in list:\n",
    "            \n",
    "            num_id = str(id)\n",
    "            \n",
    "            np_test_data = np.load(\"../../../Data/Xi_W_all/denoise_data/\"+cross+\"/\"+snr_str+\"/test_data/\" + num_id + \"_test.npy\")\n",
    "            npd_test_lab = np.load(\"../../../Data/Xi_W_all/denoise_data/\"+cross+\"/no_db/test_data/\" + num_id + \"_test.npy\") \n",
    "            npc_test_lab = np.load(\"../../../Data/Xi_W_all/denoise_data/\"+cross+\"/\"+snr_str+\"/test_lab/\" + num_id + \"_lab.npy\")\n",
    "            \n",
    "            test_data[flag, :, :] = np_test_data\n",
    "            d_test_lab[flag, :, :] = npd_test_lab    \n",
    "            c_test_lab[flag] = npc_test_lab\n",
    "            \n",
    "            flag += 1\n",
    "            \n",
    "            if flag >= test_batch:\n",
    "                \n",
    "                test_hot_lab = to_categorical(c_test_lab, num_classes=10)\n",
    "                \n",
    "                yield [test_data], [d_test_lab]\n",
    "                \n",
    "                flag = 0\n",
    "                test_data = np.zeros((test_batch, 2048, 1), dtype = np.float32)\n",
    "                d_test_lab = np.zeros((test_batch, 2048, 1), dtype = np.float32)\n",
    "                c_test_lab = np.zeros((test_batch))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_all_test_data():\n",
    "    \n",
    "    test_data = np.zeros((test_num, 2048, 1), dtype = np.float32)\n",
    "    d_test_lab = np.zeros((test_num, 2048, 1), dtype = np.float32)\n",
    "    c_test_lab = np.zeros((test_num))\n",
    "    \n",
    "    flag = 0\n",
    "    list = random.sample(range(test_num), test_num)\n",
    "    \n",
    "    for id in list:\n",
    "        \n",
    "        num_id = str(id)\n",
    "        \n",
    "        np_test_data = np.load(\"../../../Data/Xi_W_all/denoise_data/\"+cross+\"/\"+snr_str+\"/test_data/\" + num_id + \"_test.npy\")\n",
    "        npd_test_lab = np.load(\"../../../Data/Xi_W_all/denoise_data/\"+cross+\"/no_db/test_data/\" + num_id + \"_test.npy\") \n",
    "        npc_test_lab = np.load(\"../../../Data/Xi_W_all/denoise_data/\"+cross+\"/\"+snr_str+\"/test_lab/\" + num_id + \"_lab.npy\")\n",
    "            \n",
    "        test_data[flag, :, :] = np_test_data\n",
    "        d_test_lab[flag, :, :] = npd_test_lab    \n",
    "        c_test_lab[flag] = npc_test_lab\n",
    "        \n",
    "        flag += 1\n",
    "        \n",
    "    test_hot_lab = to_categorical(c_test_lab, num_classes=10)\n",
    "    \n",
    "    return test_data, d_test_lab, test_hot_lab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "slope = 0.3\n",
    "\n",
    "def Denosing_CNN2():\n",
    "    \n",
    "    input = Input(shape=[2048, 1])\n",
    "\n",
    "    D1 = Conv1D(filters=16, kernel_size=12, strides=4, padding=\"same\")(input)\n",
    "    D1 = LeakyReLU(alpha=slope)(D1)\n",
    "        \n",
    "    D2 = Conv1D(filters=32, kernel_size=6, strides=4, padding=\"same\")(D1)\n",
    "    D2 = LeakyReLU(alpha=slope)(D2)    \n",
    "    \n",
    "    D3 = Conv1D(filters=64, kernel_size=3, strides=2, padding=\"same\")(D2)\n",
    "    D3 = LeakyReLU(alpha=slope)(D3)\n",
    "    \n",
    "    D4 = Conv1D(filters=128, kernel_size=3, strides=2, padding=\"same\")(D3)\n",
    "    D4 = LeakyReLU(alpha=slope)(D4)    \n",
    "\n",
    "    \n",
    "    Dx_4 = Reshape(target_shape=[32, 1, 128])(D4)    \n",
    "    \n",
    "    \n",
    "    Dx_5 = Deconv2D(filters=128, kernel_size=[3, 1], strides=[2, 1], padding=\"same\")(Dx_4)\n",
    "    Dx_5 = LeakyReLU(alpha=slope)(Dx_5)   \n",
    "    \n",
    "    Dx_6 = Deconv2D(filters=64, kernel_size=[3, 1], strides=[2, 1], padding=\"same\")(Dx_5)\n",
    "    Dx_6 = LeakyReLU(alpha=slope)(Dx_6)\n",
    "    \n",
    "    Dx_7 = Deconv2D(filters=32, kernel_size=[6, 1], strides=[4, 1], padding=\"same\")(Dx_6)\n",
    "    Dx_7 = LeakyReLU(alpha=slope)(Dx_7)\n",
    "\n",
    "    Dx_8 = Deconv2D(filters=16, kernel_size=[12, 1], strides=[4, 1], padding=\"same\")(Dx_7)\n",
    "    Dx_8 = LeakyReLU(alpha=slope)(Dx_8)\n",
    "    \n",
    "    Dx_9 = Deconv2D(filters=1, kernel_size=[1, 1], strides=[1, 1], padding=\"same\")(Dx_8)\n",
    "    Dx_9 = LeakyReLU(alpha=slope)(Dx_9)    \n",
    "    \n",
    "    Dx_10 = Reshape(target_shape=[2048, 1], name='out1')(Dx_9)      \n",
    "\n",
    "         \n",
    "    Total_Model = Model(inputs=input, outputs=Dx_10)  \n",
    "    \n",
    "    return Total_Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_1 (InputLayer)         (None, 2048, 1)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_1 (Conv1D)            (None, 512, 16)           208       \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_1 (LeakyReLU)    (None, 512, 16)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_2 (Conv1D)            (None, 128, 32)           3104      \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_2 (LeakyReLU)    (None, 128, 32)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_3 (Conv1D)            (None, 64, 64)            6208      \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_3 (LeakyReLU)    (None, 64, 64)            0         \n",
      "_________________________________________________________________\n",
      "conv1d_4 (Conv1D)            (None, 32, 128)           24704     \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_4 (LeakyReLU)    (None, 32, 128)           0         \n",
      "_________________________________________________________________\n",
      "reshape_1 (Reshape)          (None, 32, 1, 128)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_transpose_1 (Conv2DTr (None, 64, 1, 128)        49280     \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_5 (LeakyReLU)    (None, 64, 1, 128)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_transpose_2 (Conv2DTr (None, 128, 1, 64)        24640     \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_6 (LeakyReLU)    (None, 128, 1, 64)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_transpose_3 (Conv2DTr (None, 512, 1, 32)        12320     \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_7 (LeakyReLU)    (None, 512, 1, 32)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_transpose_4 (Conv2DTr (None, 2048, 1, 16)       6160      \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_8 (LeakyReLU)    (None, 2048, 1, 16)       0         \n",
      "_________________________________________________________________\n",
      "conv2d_transpose_5 (Conv2DTr (None, 2048, 1, 1)        17        \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_9 (LeakyReLU)    (None, 2048, 1, 1)        0         \n",
      "_________________________________________________________________\n",
      "out1 (Reshape)               (None, 2048, 1)           0         \n",
      "=================================================================\n",
      "Total params: 126,641\n",
      "Trainable params: 126,641\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "adam = optimizers.adam(lr = 0.0001)\n",
    "\n",
    "model = Denosing_CNN2()\n",
    "\n",
    "model.compile(optimizer = 'adam', loss='mean_squared_error', metrics = ['mean_squared_error'])\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "class MyCbk(Callback):\n",
    "\n",
    "    def __init__(self, model):\n",
    "        self.model_to_save = model\n",
    "            \n",
    "    def on_epoch_end(self, epoch, logs=None):\n",
    "        \n",
    "        print('save model----model_at_epoch_%d.h5' % epoch)\n",
    "        self.model_to_save.save('model/2/model_%d.h5' % epoch)\n",
    "        \n",
    "cbk = MyCbk(model)\n",
    "\n",
    "reduceLR = ReduceLROnPlateau(monitor='val_loss', factor=0.90, patience=5, verbose=0, mode='auto', cooldown=0, min_lr=0.0000001)\n",
    "\n",
    "results = model.fit_generator(generate_train_data(), steps_per_epoch = train_num/train_batch, epochs = 100, validation_data = generate_test_data(), validation_steps = test_num/test_batch, verbose=1, callbacks=[cbk, reduceLR])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_test_data, all_d_lab, all_hot_lab = get_all_test_data()\n",
    "\n",
    "def SNR_score(noise_data, pure_data):\n",
    "    \n",
    "    noise_data = np.array(noise_data)\n",
    "    pure_data = np.array(pure_data)\n",
    "    \n",
    "    noise_power = np.sum((noise_data-pure_data)**2)/len(noise_data)\n",
    "    pure_power = np.sum(pure_data**2)/len(pure_data)\n",
    "    \n",
    "    SNR_d = 10*math.log10(pure_power/noise_power)\n",
    "    \n",
    "    return SNR_d\n",
    "\n",
    "\n",
    "def MSE_score(noise_data, pure_data):\n",
    "    \n",
    "    noise_data = np.array(noise_data)\n",
    "    pure_data = np.array(pure_data)\n",
    "    \n",
    "    MSE_d = np.sum((noise_data-pure_data)**2)/(len(noise_data)*2048)\n",
    "    \n",
    "    return MSE_d \n",
    "\n",
    "\n",
    "SNR_list = np.zeros((100))\n",
    "MSE_list = np.zeros((100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for model_id in range(99,100):\n",
    "    \n",
    "    str_id = str(model_id)\n",
    "    model =  load_model('model/2/model_'+ str_id +'.h5') \n",
    "    \n",
    "    range_num = int(test_num/test_batch)\n",
    "    \n",
    "    input_d_lab = []\n",
    "    output_d_lab = []\n",
    "    \n",
    "    for num_id in range(range_num):\n",
    "        \n",
    "        test_data = all_test_data[num_id*test_batch:test_batch+num_id*test_batch,:,:]\n",
    "        d_lab = all_d_lab[num_id*test_batch:test_batch+num_id*test_batch,:,:]\n",
    "    \n",
    "        pre_d = model.predict(test_data, batch_size=test_batch, verbose=0)\n",
    "\n",
    "        input_d_lab.extend(d_lab)\n",
    "        output_d_lab.extend(pre_d)\n",
    "        \n",
    "    SNR = SNR_score(output_d_lab, input_d_lab)\n",
    "    MSE = MSE_score(output_d_lab, input_d_lab)\n",
    "\n",
    "    SNR_list[model_id] = SNR\n",
    "    MSE_list[model_id] = MSE\n",
    "    \n",
    "    print(\"第\"+ str(model_id) + \"个模型的结果：\")\n",
    "    print(SNR)\n",
    "    print(MSE)\n",
    "    \n",
    "    print(\"------------------------------------\")\n",
    "    \n",
    "    K.clear_session()\n",
    "    tf.reset_default_graph()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "SNR_str = \"[\"\n",
    "MSE_str = \"[\"\n",
    "\n",
    "for id in range(80, 100):\n",
    "    \n",
    "    SNR_num = round(SNR_list[id], 5)\n",
    "    MSE_num = round(MSE_list[id], 5)\n",
    "\n",
    "    SNR_str = SNR_str + str(SNR_num)\n",
    "    MSE_str = MSE_str + str(MSE_num)\n",
    "    \n",
    "    if id == 99:\n",
    "        SNR_str = SNR_str + \"]\"\n",
    "        MSE_str = MSE_str + \"]\"\n",
    "    else:\n",
    "        SNR_str = SNR_str + \", \"\n",
    "        MSE_str = MSE_str + \", \"\n",
    "\n",
    "print(\"SNR_score:\")\n",
    "print(SNR_str)\n",
    "print(\"MSE_score:\")\n",
    "print(MSE_str)\n",
    "\n",
    "print('SNR: '+str(SNR_list[80:99].max()))\n",
    "print('MSE: '+str(MSE_list[80:99].min()))\n",
    "print(\"**------------**-------------**---------------**\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python tensorflow",
   "language": "python",
   "name": "tensorflow"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
